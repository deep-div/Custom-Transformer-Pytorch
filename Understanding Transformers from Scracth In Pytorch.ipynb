{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:39:25.462594Z","iopub.execute_input":"2025-04-18T06:39:25.462868Z","iopub.status.idle":"2025-04-18T06:39:29.962412Z","shell.execute_reply.started":"2025-04-18T06:39:25.462838Z","shell.execute_reply":"2025-04-18T06:39:29.961303Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport math\nimport copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:39:29.963444Z","iopub.execute_input":"2025-04-18T06:39:29.963691Z","iopub.status.idle":"2025-04-18T06:39:33.313851Z","shell.execute_reply.started":"2025-04-18T06:39:29.963670Z","shell.execute_reply":"2025-04-18T06:39:33.313130Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# 🔹 Understanding Self-Attention in Multi-Head Attention\n\n\n## **Step 1: Tokenization and Embedding**  \nBefore attention is applied, each word in the sentence is **tokenized**, then converted into **embeddings**, concatenated with its **positional encoding**, and finally represented as a numerical vector (**embedding**).\n\n- In the *\"Attention Is All You Need\"* paper, `d_model = 512`. Each word is represented as a **1D vector of 512 dimensions (512D)**.\n\n### **Example Sentence:**\n> **\"Hi how are you\"**\n\nEach word is converted into a vector:\n\n- **\"Hi\"** → `[0.1, 0.3, 0.7, ...]`  (Size: `d_model`)\n- **\"how\"** → `[0.2, 0.6, 0.5, ...]`  (Size: `d_model`)\n- **\"are\"** → `[0.4, 0.2, 0.8, ...]`  (Size: `d_model`)\n- **\"you\"** → `[0.9, 0.1, 0.3, ...]`  (Size: `d_model`)\n\nAt this point, each word is just an **embedding vector of size `d_model`**.\n\n> **Note:** Positional encoding is **added** to each embedding to retain word order information.\n\n---\n\n## **Step 2: Creating Query (Q), Key (K), and Value (V)**\nFor **each word**, we create three separate vectors:\n\n- **Query (Q)** → Determines how much focus a word should get.\n- **Key (K)** → Helps decide how much attention a word receives from other words.\n- **Value (V)** → Contains the actual word information to be passed on.\n\nThese vectors are computed using linear transformations:\n\nQ = W_q * X\n\n\nK = W_k * X\n\n\nV = W_v * X\n\nwhere \\( W_q, W_k, W_v \\) are **learnable weight matrices**. Whereas, X represents the input embeddings of the words/tokens in the sentence.\n\nFor example, for the word **\"Hi\"**, we get:\n- Query vector **Q_hi** (size: `d_model` = 512D)\n- Key vector **K_hi** (size: `d_model` = 512D)\n- Value vector **V_hi** (size: `d_model` = 512D)\n\nThis same process applies to all other words.\n\n---\n\n## **Step 3: Compute Attention Scores in One Head**\nNow, we compare how much each word should **attend to** every other word in the sentence. This is done by computing the **dot product** between the **query of one word** and the **keys of all words**.\n\nSince **multi-head attention** is used, each head works with a smaller subspace:\n- `d_k = d_model / num_heads`\n- If `d_model = 512` and `num_heads = 8`, then `d_k = 512 / 8 = 64`.\n- Each head gets **Q, K, V vectors of size 64D**.\n\n| Word  | Query (Q)  | Key (K)  | Value (V)  |\n|-------|-----------|----------|------------|\n| Hi    | Q_hi (64D) | K_hi (64D) | V_hi (64D) |\n| How   | Q_how (64D) | K_how (64D) | V_how (64D) |\n| Are   | Q_are (64D) | K_are (64D) | V_are (64D) |\n| You   | Q_you (64D) | K_you (64D) | V_you (64D) |\n\n### **Step 3.1: Compute Raw Scores**\nFor word **Hi**, we compute the dot product of its query with the keys of all words:\n\nScore1= Q_hi.K_hi\n\nScore2= Q_hi.K_how\n\nScore3= Q_hi.K_are\n\nScore4= Q_hi.K_you\n\n### **Step 3.2: Apply Softmax**\nThe scores are **scaled** to avoid large gradients by dividing by \\( \\sqrt{d_k} \\) and then passed through **softmax** to get probabilities:\n\n$$\n\\text{Attention Weight} = \\text{softmax} \\left( \\frac{Q \\cdot K^T}{\\sqrt{d_k}} \\right)\n$$\n\n$$\n\\text{Attention Weight}_i = \\text{softmax} \\left(Score_i\\right)\n$$\n\nEach word now has an **attention score** that tells how much focus it should give to the other words.\n\n### **Step 3.3: Compute Final Weighted Sum**\nMultiply each attention weight by the corresponding **Value (V) vector**:\n\nOutput1= Weight1*V_hi\n\nOutput2= Weight2*V_how\n\nOutput3= Weight2*V_are\n\nOutput4= Weight4*V_you\n\nFinal embedding for the word **Hi** is: Output1 + Output2 + Output3 + Output4 \n\nEach output is **64D**, so we get **one 64D vector per word per attention head**.\n\n> **Note:** We have Calculated Final Contexual Embeddings of word Hi. Similarly, we have to for other words how, are, you. This is done by computing the **dot product** between the **query of one word** and the **keys of all words**.\n---\n\n## **Step 4: Multi-Head Attention**\nSince we have **8 heads**, each computes **separate self-attention** and gives an output of size **(batch, seq_length, d_k) = (1, 4, 64)**.\n\nAfter processing all heads, their outputs are **concatenated** to restore the original `d_model = 512`: **(1, 4, 512)**\n\nThe final **multi-head attention output** has the same size as the input embeddings (`d_model = 512`).\n\n","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads): \n        #d_model are dimensions of embeddings for each token or word in sentence. In Attention is all you need paper they used 512D 1D vector size.\n        #num_heads is self attention heads used in multi head attention. In Attention is all you need paper they used 8 self attention heads to make a multi head mechanisim\n        super(MultiHeadAttention, self).__init__() # Calls the constructor of the parent class (nn.Module).\n        # Ensure that the model dimension (d_model) is divisible by the number of heads\n        # This is important because each head needs an equal portion of the total embedding space.\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        # Initialize dimensions\n        self.d_model = d_model # Embedding Model dimension\n        self.num_heads = num_heads # Number of attention heads\n        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n        \n        #  Define Learnable Linear Parameters with input and outputs layer\n        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Calculate attention scores\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # Formula to calculate attention scores which comes from dot product\n\n        #If mask == 0, it replaces the attention score with -1e9, making softmax output close to 0. Also called Masked Multihead Attention\n        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9) # if mask is not none we add mask too small that its softmax is zero\n        \n        # Softmax is applied to obtain attention probabilities \n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        \n        # Multiply by values to obtain the final output vector\n        output = torch.matmul(attn_probs, V) \n        return output \n\n    # TO convert 512D vector into 64D vector per head so we can run all 8 heads parellel.\n    #This function reshapes the input from (batch_size, seq_len, d_model) to (batch_size, num_heads, seq_len, d_k) where d_k = d_model / num_heads.\n    def split_heads(self, x):\n        # Reshape the input to have num_heads for multi-head attention\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n\n    # Combine the multiple heads back to original shape which was 512D. We get 512D final embeddings for each word.\n    def combine_heads(self, x):\n        batch_size, _, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n\n    #putting all the functions together\n    def forward(self, Q, K, V, mask=None):\n        # Apply linear transformations and split heads\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n        \n        # Perform scaled dot-product attention\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Combine heads and apply output transformation\n        output = self.W_o(self.combine_heads(attn_output))\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:39:36.034065Z","iopub.execute_input":"2025-04-18T06:39:36.034463Z","iopub.status.idle":"2025-04-18T06:39:36.046781Z","shell.execute_reply.started":"2025-04-18T06:39:36.034430Z","shell.execute_reply":"2025-04-18T06:39:36.045836Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 🔹Understanding FeedForward In Transformer\n\nThe Position-Wise Feed Forward Network (FFN) is a key component of the Transformer architecture.\nIt is applied independently to each token in the sequence after multi-head attention.\n\n- Multi-head attention captures **relationships between words**, but it does **not change individual word representations much**.  \n- The **FFN introduces non-linearity and richer transformations** to enhance each token’s representation.  \n- It consists of **two linear transformations** with a **ReLU activation** in between.\n","metadata":{}},{"cell_type":"code","source":"class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        #d_ff: The hidden layer size (usually larger, e.g., 2048 in the original Transformer).\n        # we have 4 words in sentence \"Hi How Are You\" so each word will be expaned to 2048. The input, hidden, output layers (512 → 2048 → 512)\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU() # Activation Function\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:40:32.544012Z","iopub.execute_input":"2025-04-18T06:40:32.544296Z","iopub.status.idle":"2025-04-18T06:40:32.549126Z","shell.execute_reply.started":"2025-04-18T06:40:32.544276Z","shell.execute_reply":"2025-04-18T06:40:32.548433Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 🔹 Positional Encoding\n\n##  How Positional Encoding Works?\nSince self-attention treats all words **independently**, it doesn't understand their order. Positional encoding assigns each position a unique vector, ensuring the model understands **word order**.\n\n### **Step 1: Convert Words to Word Embeddings**\nBefore adding positional encoding, each word gets converted into a **512-dimensional vector** using an embedding layer.\n\nLet's assume our embedding model has an **embedding size (`d_model`) of 512**.\n\n| Token  | Word Embedding (Simplified: 3D instead of 512D) |\n|--------|--------------------------------|\n| **Hi**  | `[0.3, 0.5, -0.2]`  |\n| **How** | `[0.7, -0.1, 0.9]`  |\n| **Are** | `[-0.5, 0.3, 0.6]`  |\n| **You** | `[0.1, -0.4, 0.8]`  |\n\n---\n\n### **Step 2: Generate Unique Positional Encoding**\nEach **position** (0, 1, 2, 3) is assigned a **unique vector** using a combination of **sine and cosine functions** at different frequencies.\n\n#### **Formula:**\nEach position `p` (word index) is assigned a **512-dimensional vector** using:\n\n$$\nPE(p, 2i) = \\sin\\left(\\frac{p}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n$$\n\n$$\nPE(p, 2i+1) = \\cos\\left(\\frac{p}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n$$\n\n\nwhere:\n- **`p`** = Position index (0 for \"Hi\", 1 for \"How\", etc.)\n- **`i`** = Dimension index (half use `sin`, half use `cos`)\n- **`d_model`** = Embedding size (e.g., 512)\n- **10000** = A constant to control frequency scaling\n\nFor **simplicity**, let's assume `d_model = 6` instead of 512:\n\n| Position `p` | PE(0) (sin) | PE(1) (cos) | PE(2) (sin) | PE(3) (cos) | PE(4) (sin) | PE(5) (cos) |\n|-------------|------------|------------|------------|------------|------------|------------|\n| **0** (Hi)  | `0.0000`  | `1.0000`  | `0.0000`  | `1.0000`  | `0.0000`  | `1.0000`  |\n| **1** (How) | `0.8415`  | `0.5403`  | `0.4207`  | `0.9070`  | `0.2104`  | `0.9775`  |\n| **2** (Are) | `0.9093`  | `-0.4161` | `0.6543`  | `0.7561`  | `0.3784`  | `0.9256`  |\n| **3** (You) | `0.1411`  | `-0.9900` | `0.8415`  | `0.5403`  | `0.5000`  | `0.8660`  |\n\nEach position receives **a unique vector**, ensuring that different words have different encodings.\n\n---\n\n### **Step 3: Add Positional Encoding to Word Embeddings**\nEach word’s embedding is **element-wise added** to its corresponding positional encoding.\n\n| Token  | Word Embedding | Positional Encoding | **Final Embedding (Word + PE)** |\n|--------|-----------------|-----------------|------------------|\n| **Hi**  | `[0.3, 0.5, -0.2]`  | `[0.00, 1.00, 0.00]`  | `[0.3, 1.5, -0.2]` |\n| **How** | `[0.7, -0.1, 0.9]`  | `[0.84, 0.54, 0.42]`  | `[1.54, 0.44, 1.32]` |\n| **Are** | `[-0.5, 0.3, 0.6]`  | `[0.91, -0.41, 0.65]`  | `[0.41, -0.11, 1.25]` |\n| **You** | `[0.1, -0.4, 0.8]`  | `[0.14, -0.99, 0.84]`  | `[0.24, -1.39, 1.64]` |\n\n---\n","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        #max_seq_length: The maximum length of the input sequence (i.e., the longest sequence that the model will process).\n        \n        super(PositionalEncoding, self).__init__()\n\n        #Example: If d_model = 512 and max_seq_length = 100, each element in a sequence of length up to 100 will be represented as a 512-dimensional vector.\n        #Initializes a tensor pe with zeros, with dimensions (max_seq_length, d_model). This will store the positional encoding values.\n        pe = torch.zeros(max_seq_length, d_model)\n        #Creates a tensor position that represents the position of each token in the sequence \n        #unsqueeze(1) adds an additional dimension to make it a column vector of shape (max_seq_length, 1).\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n        \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:08:38.677393Z","iopub.execute_input":"2025-04-17T08:08:38.677695Z","iopub.status.idle":"2025-04-17T08:08:38.683366Z","shell.execute_reply.started":"2025-04-17T08:08:38.677669Z","shell.execute_reply":"2025-04-17T08:08:38.682413Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 🔹 Encoder Block\n\n<p align=\"center\">\n  <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*7sjcgd_nyODdLbZSxyxz_g.png\" width=\"300\"/>\n</p>\n\n","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        # d_model: the dimensionality of the input and output vectors (the embedding size).\n        # num_heads: the number of self attention heads for the multi-head attention mechanism.\n        #d_ff: the size of the feed-forward network's hidden layer.\n        #dropout: the dropout rate to be applied for regularization\n        \n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads) # calls Constructor of Multihead class gives Wq,Wk,Wv,Wo\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    \n    def forward(self, x, mask):\n        attn_output = self.self_attn(x, x, x, mask) # For better details check Dummy Encoder Run\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:08:41.738351Z","iopub.execute_input":"2025-04-17T08:08:41.738669Z","iopub.status.idle":"2025-04-17T08:08:41.744271Z","shell.execute_reply.started":"2025-04-17T08:08:41.738640Z","shell.execute_reply":"2025-04-17T08:08:41.743431Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Dummy Encoder Run","metadata":{}},{"cell_type":"code","source":"# 🚀 Step 1: Create Encoder Layer\nencoder_layer = EncoderLayer(d_model=512, num_heads=8, d_ff=2048, dropout=0.1)\nprint(\"Self Attention Module:\\n\", encoder_layer.self_attn)\n\n# 🚀 Step 2: Create Dummy Input Data which is K,Q,V coming from constructor method of MultiHead Class\nbatch_size = 2\nseq_length = 10\nd_model = 512\n\nx = torch.randn(batch_size, seq_length, d_model)  # Random input tensor\nmask = torch.ones(batch_size, 1, 1, seq_length)  # Example mask (all ones means no masking in encoder)\nprint(\"Random Tenser X:\", x)\nprint(\"Mask:\", mask)\n\n# 🚀 Step 3: Directly call self.self_attn like inside EncoderLayer \n# implicitly calling the forward() method of the MultiHeadAttention class because it's a subclass of nn.Module.\nattn_output = encoder_layer.self_attn(x, x, x, mask) # x is here Q,K,V which is calculated by constructor method of multiheadattention\nprint(\"Attention Output Shape:\", attn_output.shape)  # Expected: (batch_size, seq_length, d_model)\n\n# 🚀 Step 4: Apply Dropout on attn_output\ndropout_output = encoder_layer.dropout(attn_output) # x + self.dropout(attn_output)\n\n# 🚀 Step 5: Residual Connection (Adding x + dropout_output)\nresidual_output = x + dropout_output \n\n# 🚀 Step 6: Apply Layer Normalization\nnormalized_output = encoder_layer.norm1(residual_output) #self.norm1(x + self.dropout(attn_output))\nprint(\"Final Output Shape After Norm1:\", normalized_output.shape)  # Expected: (batch_size, seq_length, d_model)\n \n# 🚀 Step 7: Apply PositionWiseFeedForward ## Feed Forward\nPositionWise_FFN = PositionWiseFeedForward(d_model, d_ff=2048)\nprint(\"\\nFeed Forward Network:\\n\", PositionWise_FFN)\n\n# 🚀 Step 8: Forward pass through Feed-Forward Network  \nffn_output = PositionWise_FFN.forward(normalized_output)\nprint(\"Feed Forward Output Shape:\", ffn_output.shape)  # Expected: (batch_size, seq_length, d_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:08:44.562118Z","iopub.execute_input":"2025-04-17T08:08:44.562393Z","iopub.status.idle":"2025-04-17T08:08:44.797695Z","shell.execute_reply.started":"2025-04-17T08:08:44.562372Z","shell.execute_reply":"2025-04-17T08:08:44.796916Z"}},"outputs":[{"name":"stdout","text":"Self Attention Module:\n MultiHeadAttention(\n  (W_q): Linear(in_features=512, out_features=512, bias=True)\n  (W_k): Linear(in_features=512, out_features=512, bias=True)\n  (W_v): Linear(in_features=512, out_features=512, bias=True)\n  (W_o): Linear(in_features=512, out_features=512, bias=True)\n)\nRandom Tenser X: tensor([[[ 1.3470,  0.8165,  0.8658,  ...,  0.4236,  0.4099, -0.2743],\n         [ 1.2221, -0.4489, -1.1829,  ...,  0.0461,  0.9481, -0.2679],\n         [ 0.4355,  0.2548,  1.1793,  ..., -1.2696, -1.1451, -1.1789],\n         ...,\n         [ 0.8899, -1.7102, -1.1989,  ..., -0.6538,  1.1413,  0.5932],\n         [-2.0163, -0.8217, -0.2244,  ..., -0.6873,  0.3968, -0.9653],\n         [ 0.7070, -1.2205, -1.8267,  ...,  1.0535, -0.1410,  0.6317]],\n\n        [[-0.2732,  1.2016, -0.1432,  ...,  0.0168,  0.2845,  1.0413],\n         [-0.1246,  0.6747, -1.3493,  ..., -0.9134, -0.6291, -0.7386],\n         [-0.5668,  0.0650, -0.0444,  ..., -0.1624,  1.0665, -0.3356],\n         ...,\n         [ 0.4902,  0.2188, -1.1691,  ...,  0.0100,  0.7311,  0.2696],\n         [ 0.5985,  0.0293, -0.7626,  ...,  0.4511,  2.8798, -0.0635],\n         [ 1.2052, -0.0076,  1.3974,  ...,  1.2898,  1.0269,  1.3152]]])\nMask: tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n\n\n        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\nAttention Output Shape: torch.Size([2, 10, 512])\nFinal Output Shape After Norm1: torch.Size([2, 10, 512])\n\nFeed Forward Network:\n PositionWiseFeedForward(\n  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n  (relu): ReLU()\n)\nFeed Forward Output Shape: torch.Size([2, 10, 512])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# 🔹 Decoder Block\n\n<p align=\"center\">\n  <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*vYgZyhNOoPKdeSEnN1i9Kg.png\">\n</p>\n\n","metadata":{}},{"cell_type":"markdown","source":"# 🔹 Understanding Masked Self-Attention in Multi-Head Attention\n\n\n## 1. Translation and Tokens\n\n- **English:** “Hi how are you”  [this we pass from encoder block]\n- **Hindi:** “हाय कैसे हो तुम”  [ this we pass in decoder block while training so it is non autogressive in training]\n\nToken sequence (4 tokens):\n```text\n[\"हाय\", \"कैसे\", \"हो\", \"तुम\"]\n```\n\n---\n\n## 2. Q, K, V Matrices\n\nWe stack each token’s Q/K/V vectors into 4×4 matrices (rows = tokens, cols = d_model = 4): [in attention paper we have d_model 512]\n\nQ = \n\\begin{bmatrix}\n0.1 & 0.2 & 0.3 & 0.4 \\\\\n0.2 & 0.1 & 0.4 & 0.3 \\\\\n0.3 & 0.4 & 0.2 & 0.1 \\\\\n0.4 & 0.3 & 0.1 & 0.2\n\\end{bmatrix},\n\nK = \n\\begin{bmatrix}\n0.4 & 0.3 & 0.2 & 0.1 \\\\\n0.5 & 0.3 & 0.6 & 0.1 \\\\\n0.6 & 0.4 & 0.5 & 0.2 \\\\\n0.1 & 0.2 & 0.3 & 0.5\n\\end{bmatrix}\nV = \n\\begin{bmatrix}\n0.1 & 0.5 & 0.2 & 0.4 \\\\\n0.3 & 0.7 & 0.4 & 0.1 \\\\\n0.2 & 0.3 & 0.5 & 0.3 \\\\\n0.6 & 0.4 & 0.3 & 0.2\n\\end{bmatrix}\n\n1st row of Q,K,V → हाय  \n2nd row of Q,K,V → कैसे  \n3rd row of Q,K,V → हो  \n4th row of Q,K,V → तुम  \n\n\n\n\n## 3. Raw Attention Scores  \nCompute  \n\n$$\nS = \\left( \\frac{Q \\cdot K^T}{\\sqrt{d_k}} \\right)\n$$\n\nso that each row *i* contains dot‑products of token *i*’s Q with every token’s K:\n\n\nS = \n\\begin{bmatrix}\n0.20 & 0.33 & 0.37 & 0.34 \\\\   \n0.22 & 0.40 & 0.42 & 0.31 \\\\   \n0.29 & 0.40 & 0.46 & 0.22 \\\\  \n0.29 & 0.37 & 0.45 & 0.23     \n\\end{bmatrix}\n\n1st row has scores for → हाय  \n2nd row has scores for → कैसे  \n3rd row has scores for → हो  \n4th row has scores for → तुम \n\n## 4. Causal Mask  \nEnforce autoregressive order by masking out future positions(at हाय we dont know rest of words so we mask them with -inf as softmax of -inf is 0, same for all other words) (setting them to –∞):\n\n\nMask =\n\\begin{bmatrix}\n0      & -\\infty & -\\infty & -\\infty \\\\  % हाय (i=0)\n0      & 0       & -\\infty & -\\infty \\\\  % कैसे (i=1)\n0      & 0       & 0       & -\\infty \\\\  % हो   (i=2)\n0      & 0       & 0       & 0        % तुम  (i=3)\n\\end{bmatrix}\n\n\nAdd to *S* to get masked scores *S′*:\n\n\nS' = S + Mask =\n\\begin{bmatrix}\n0.20 & -\\infty & -\\infty & -\\infty \\\\\n0.22 & 0.40    & -\\infty & -\\infty \\\\\n0.29 & 0.40    & 0.46    & -\\infty \\\\\n0.29 & 0.37    & 0.45    & 0.23\n\\end{bmatrix}\n\n## 5. Softmax → Attention Weights  \nApply softmax **row‑wise** (ignore –∞ entries, which become zero):\n\n\nW =\n\\begin{bmatrix}\n1.000 & 0     & 0     & 0     \\\\[6pt]\n0.455 & 0.545 & 0     & 0     \\\\[6pt]\n0.303 & 0.338 & 0.359 & 0     \\\\[6pt]\n0.238 & 0.258 & 0.280 & 0.224\n\\end{bmatrix}\n\n\n- **Row “हाय”**: attends only to itself → `[1, 0, 0, 0]`  \n- **Row “कैसे”**: softmax\\((0.22,0.40)\\approx(0.455,0.545)\\)  \n- **Row “हो”**: softmax\\((0.29,0.40,0.46)\\approx(0.303,0.338,0.359)\\)  \n- **Row “तुम”**: softmax\\((0.29,0.37,0.45,0.23)\\approx(0.238,0.258,0.280,0.224)\\)\n\n\n\n## 6. Final Output  \nCompute the contextualized vectors by multipling weights by Value vector:\n\n\nO = W \\times V\n=\n\\begin{bmatrix}\n1\\cdot V_{\\text{हाय}} \\\\[4pt]\n0.455\\,V_{\\text{हाय}} + 0.545\\,V_{\\text{कैसे}} \\\\[4pt]\n0.303\\,V_{\\text{हाय}} + 0.338\\,V_{\\text{कैसे}} + 0.359\\,V_{\\text{हो}} \\\\[4pt]\n0.238\\,V_{\\text{हाय}} + 0.258\\,V_{\\text{कैसे}} + 0.280\\,V_{\\text{हो}} + 0.224\\,V_{\\text{तुम}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0.10   & 0.50   & 0.20   & 0.40   \\\\[4pt]\n0.209  & 0.609  & 0.309  & 0.237  \\\\[4pt]\n0.2035 & 0.4958 & 0.3753 & 0.2627 \\\\[4pt]\n0.2916 & 0.4732 & 0.3580 & 0.2498\n\\end{bmatrix}\n\n\n- **Output “हाय”** = `[0.10, 0.50, 0.20, 0.40]`  \n- **Output “कैसे”** ≈ `[0.209, 0.609, 0.309, 0.237]`  \n- **Output “हो”**   ≈ `[0.2035, 0.4958, 0.3753, 0.2627]`  \n- **Output “तुम”**  ≈ `[0.2916, 0.4732, 0.3580, 0.2498]`\n\n","metadata":{}},{"cell_type":"markdown","source":"# 🔹 Understanding Cross‑Attention in Encoder–Decoder Attention\n\nBelow is a step‑by‑step worked example of **cross‑attention** between an English source (“Hi how are you”) and its Hindi translation (“हाय कैसे हो तुम”), using toy matrices (with model dimension d_model = 4(In Attention paper we have 512D vector)).\n\n\n## 1. Source & Target Tokens\n\n- **Encoder (English):**  \n  “Hi how are you”  \n  → Tokens:  \n  ```text\n  [\"Hi\", \"how\", \"are\", \"you\"]\n  ```\n\n- **Decoder (Hindi):**  \n  “हाय कैसे हो तुम”  \n  → Tokens (feeding in at one time during training):  \n  ```text\n  [\"हाय\", \"कैसे\", \"हो\", \"तुम\"]\n  ```\n\n\n## 2. Q, K, V Matrices\n\n- **Queries** come from the **decoder** hidden states (one per Hindi token).  \n  Q (from decoder) =\n  \\begin{bmatrix}\n    0.1 & 0.2 & 0.3 & 0.4 \\\\ \n    0.2 & 0.1 & 0.4 & 0.3 \\\\ \n    0.3 & 0.4 & 0.2 & 0.1 \\\\ \n    0.4 & 0.3 & 0.1 & 0.2\n  \\end{bmatrix}\n  – Row 1 → हाय  \n  – Row 2 → कैसे  \n  – Row 3 → हो  \n  – Row 4 → तुम  \n\n- **Keys** and **Values** come from the **encoder** outputs (one per English token).  \n  K (from encoder) =\n  \\begin{bmatrix}\n    0.4 & 0.3 & 0.2 & 0.1 \\\\ \n    0.5 & 0.3 & 0.6 & 0.1 \\\\ \n    0.6 & 0.4 & 0.5 & 0.2 \\\\ \n    0.1 & 0.2 & 0.3 & 0.5\n  \\end{bmatrix},\n  V (from encoder) =\n  \\begin{bmatrix}\n    0.1 & 0.5 & 0.2 & 0.4 \\\\ \n    0.3 & 0.7 & 0.4 & 0.1 \\\\ \n    0.2 & 0.3 & 0.5 & 0.3 \\\\ \n    0.6 & 0.4 & 0.3 & 0.2\n  \\end{bmatrix}\n  – Row 1 → “Hi”  \n  – Row 2 → “how”  \n  – Row 3 → “are”  \n  – Row 4 → “you”  \n\n\n## 3. Raw Cross‑Attention Scores\n\nCompute:\n$$\nS = \\left( \\frac{Q \\cdot K^T}{\\sqrt{d_k}} \\right)\n$$\n\nS =\n\\begin{bmatrix}\n0.20 & 0.33 & 0.37 & 0.34 \\\\  \n0.22 & 0.40 & 0.42 & 0.31 \\\\  \n0.29 & 0.40 & 0.46 & 0.22 \\\\  \n0.29 & 0.37 & 0.45 & 0.23\n\\end{bmatrix}\n\n- Row 1 (“हाय”) scores: `[0.20, 0.33, 0.37, 0.34]`  \n- Row 2 (“कैसे”) scores: `[0.22, 0.40, 0.42, 0.31]`  \n- Row 3 (“हो”)   scores: `[0.29, 0.40, 0.46, 0.22]`  \n- Row 4 (“तुम”)  scores: `[0.29, 0.37, 0.45, 0.23]`  \n\n\n## 4. Softmax → Attention Weights\n\n$$\n\\text{Attention Weight}(W) = \\text{softmax} \\left( \\frac{Q \\cdot K^T}{\\sqrt{d_k}} \\right)\n$$\n\n$$\n\\text{Attention Weight}_i(W) = \\text{softmax} \\left(Score_i\\right)\n$$\n\n\\begin{bmatrix}\n0.223 & 0.254 & 0.265 & 0.259 \\\\[4pt]\n0.222 & 0.265 & 0.271 & 0.242 \\\\[4pt]\n0.236 & 0.264 & 0.279 & 0.221 \\\\[4pt]\n0.238 & 0.258 & 0.280 & 0.224\n\\end{bmatrix}\n\n- **Row “हाय”**: attends most to “are” (0.265) and “you” (0.259)  \n- **Row “तुम”**: attends most to “are” (0.280)  \n\n\n## 5. Contextualized Outputs (with explicit weighted sums)\n\nW =\n\\begin{bmatrix}\n0.223 & 0.254 & 0.265 & 0.259 \\\\[4pt]\n0.222 & 0.265 & 0.271 & 0.242 \\\\[4pt]\n0.236 & 0.264 & 0.279 & 0.221 \\\\[4pt]\n0.238 & 0.258 & 0.280 & 0.224\n\\end{bmatrix},\nV (from encoder)=\n\\begin{bmatrix}\nV_{\\text{Hi}}  = [0.1,\\,0.5,\\,0.2,\\,0.4] \\\\[3pt]\nV_{\\text{how}} = [0.3,\\,0.7,\\,0.4,\\,0.1] \\\\[3pt]\nV_{\\text{are}} = [0.2,\\,0.3,\\,0.5,\\,0.3] \\\\[3pt]\nV_{\\text{you}} = [0.6,\\,0.4,\\,0.3,\\,0.2]\n\\end{bmatrix}\n\nEach output row is a weighted sum of the encoder values:\n\n1. **“हाय”**  \n   \\begin{aligned}\n   O_{\\text{हाय}}\n   &= 0.223\\,V_{\\text{Hi}}\n     + 0.254\\,V_{\\text{how}}\n     + 0.265\\,V_{\\text{are}}\n     + 0.259\\,V_{\\text{you}} \\\\[4pt]\n   &\\approx [0.307,\\,0.472,\\,0.356,\\,0.246]\n   \\end{aligned}\n\n2. **“कैसे”**  \n   \\begin{aligned}\n   O_{\\text{कैसे}}\n   &= 0.222\\,V_{\\text{Hi}}\n     + 0.265\\,V_{\\text{how}}\n     + 0.271\\,V_{\\text{are}}\n     + 0.242\\,V_{\\text{you}} \\\\[4pt]\n   &\\approx [0.301,\\,0.475,\\,0.359,\\,0.245]\n   \\end{aligned}\n\n3. **“हो”**  \n   \\begin{aligned}\n   O_{\\text{हो}}\n   &= 0.236\\,V_{\\text{Hi}}\n     + 0.264\\,V_{\\text{how}}\n     + 0.279\\,V_{\\text{are}}\n     + 0.221\\,V_{\\text{you}} \\\\[4pt]\n   &\\approx [0.291,\\,0.475,\\,0.359,\\,0.249]\n   \\end{aligned}\n\n4. **“तुम”**  \n   \\begin{aligned}\n   O_{\\text{तुम}}\n   &= 0.238\\,V_{\\text{Hi}}\n     + 0.258\\,V_{\\text{how}}\n     + 0.280\\,V_{\\text{are}}\n     + 0.224\\,V_{\\text{you}} \\\\[4pt]\n   &\\approx [0.292,\\,0.473,\\,0.358,\\,0.250]\n   \\end{aligned}\n\nO= \n\\begin{bmatrix}\n0.307 & 0.472 & 0.356 & 0.246 \\\\[4pt]\n0.301 & 0.475 & 0.359 & 0.245 \\\\[4pt]\n0.291 & 0.475 & 0.359 & 0.249 \\\\[4pt]\n0.292 & 0.473 & 0.358 & 0.250\n\\end{bmatrix}\n\n- Row 1 (“हाय”)   \n- Row 2 (“कैसे”) `  \n- Row 3 (“हो”)     \n- Row 4 (“तुम”)  `  ","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads) # autocall constructor of MultiHeadClass we get K,Q,V,O\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        attn_output = self.self_attn(x, x, x, tgt_mask)  ## we are providing here masking it is masked multihead attention\n        x = self.norm1(x + self.dropout(attn_output))\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:42:42.400876Z","iopub.execute_input":"2025-04-18T06:42:42.401144Z","iopub.status.idle":"2025-04-18T06:42:42.407396Z","shell.execute_reply.started":"2025-04-18T06:42:42.401125Z","shell.execute_reply":"2025-04-18T06:42:42.406438Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# import torch\n\n# batch_size = 2\n# seq_len = 10\n# d_model = 512\n\n# x = torch.randn(batch_size, seq_len, d_model)  # Random dummy input which will be actually Q,k,V\n\n# obj = MultiHeadAttention(d_model=512, num_heads=8)\n# output = obj.forward(x, x, x, mask=None)\n\n# print(output.shape)  # Should be (2, 10, 512)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:42:42.918093Z","iopub.execute_input":"2025-04-18T06:42:42.918382Z","iopub.status.idle":"2025-04-18T06:42:42.921721Z","shell.execute_reply.started":"2025-04-18T06:42:42.918358Z","shell.execute_reply":"2025-04-18T06:42:42.920837Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Dummy Decoder Run","metadata":{}},{"cell_type":"code","source":"# 🚀 Step 1: Create Decoder Layer\ndecoder_layer = DecoderLayer(d_model=512, num_heads=8, d_ff=2048, dropout=0.1)\nprint(\"Self Attention Module:\\n\", decoder_layer.self_attn) \nprint(\"Cross Attention Module:\\n\", decoder_layer.cross_attn)\n\n# 🚀 Step 2: Create Dummy Input Data\nbatch_size = 2\nseq_length = 10\nd_model = 512\n\n# Target (Decoder) Input\nx = torch.randn(batch_size, seq_length, d_model)  # Random dummy input which will be actually Q,k,V\n\n# Source (Encoder) Output \nenc_output = torch.randn(batch_size, seq_length, d_model)  # Dummy encoder output\n\n# Masks\nsrc_mask = torch.ones(batch_size, 1, 1, seq_length)         # Source mask (for encoder output)\ntgt_mask = torch.tril(torch.ones(seq_length, seq_length))  # Target mask (causal mask for decoder input) #at each time step we dont know the output so we put all future words are zero\ntgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)              # Reshape to match attention dimensions\n\nprint(\"x:\", x)\nprint(\"enc_output:\", enc_output)\nprint(\"src_mask:\", src_mask) # [\"I am learning transformers . [PAD] [PAD] [PAD]\"] # src_mask = [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]\nprint(\"tgt_mask:\", tgt_mask)\n\n# 🚀 Step 3: Self-Attention in Decoder\nattn_output = decoder_layer.self_attn(x, x, x, tgt_mask)\nprint(\"Self Attention Output Shape:\", attn_output.shape)\n\n# 🚀 Step 4: Apply Dropout on Self-Attention Output\ndropout_output = decoder_layer.dropout(attn_output)\n\n# 🚀 Step 5: Residual Connection\nresidual_output = x + dropout_output\n\n# 🚀 Step 6: LayerNorm after Self-Attention\nnormalized_output = decoder_layer.norm1(residual_output)\nprint(\"Output After Norm1:\", normalized_output.shape)\n\n# 🚀 Step 7: Cross-Attention with Encoder Output\nattn_output = decoder_layer.cross_attn(normalized_output, enc_output, enc_output, src_mask)\nprint(\"Cross Attention Output Shape:\", attn_output.shape)\n\n# 🚀 Step 8: Residual Connection for Cross-Attention\nresidual_output = normalized_output + decoder_layer.dropout(attn_output)\n\n# 🚀 Step 9: LayerNorm after Cross-Attention\nnormalized_output = decoder_layer.norm2(residual_output)\nprint(\"Output After Norm2:\", normalized_output.shape)\n\n# 🚀 Step 10: Feed-Forward Network\nffn_output = decoder_layer.feed_forward(normalized_output)\nprint(\"Feed Forward Output Shape:\", ffn_output.shape)\n\n# 🚀 Step 11: Residual Connection After FFN\nresidual_output = normalized_output + decoder_layer.dropout(ffn_output)\n\n# 🚀 Step 12: Final Layer Normalization\nfinal_output = decoder_layer.norm3(residual_output)\nprint(\"Final Output Shape After Norm3:\", final_output.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T06:42:46.153100Z","iopub.execute_input":"2025-04-18T06:42:46.153448Z","iopub.status.idle":"2025-04-18T06:42:46.397004Z","shell.execute_reply.started":"2025-04-18T06:42:46.153420Z","shell.execute_reply":"2025-04-18T06:42:46.395979Z"}},"outputs":[{"name":"stdout","text":"Self Attention Module:\n MultiHeadAttention(\n  (W_q): Linear(in_features=512, out_features=512, bias=True)\n  (W_k): Linear(in_features=512, out_features=512, bias=True)\n  (W_v): Linear(in_features=512, out_features=512, bias=True)\n  (W_o): Linear(in_features=512, out_features=512, bias=True)\n)\nCross Attention Module:\n MultiHeadAttention(\n  (W_q): Linear(in_features=512, out_features=512, bias=True)\n  (W_k): Linear(in_features=512, out_features=512, bias=True)\n  (W_v): Linear(in_features=512, out_features=512, bias=True)\n  (W_o): Linear(in_features=512, out_features=512, bias=True)\n)\nx: tensor([[[-0.7171,  0.3262, -1.0513,  ..., -0.0124,  0.4913, -0.0346],\n         [ 1.5408,  1.4473,  0.8066,  ...,  0.9618,  1.1064, -1.0759],\n         [ 0.6266,  1.2028, -1.7829,  ...,  0.6074,  0.4523, -0.3445],\n         ...,\n         [ 1.0011,  1.0257, -0.9379,  ...,  0.5158,  1.7313,  0.4191],\n         [ 1.1177,  0.0554, -0.8258,  ...,  0.1114, -1.8023,  0.7312],\n         [-0.9025,  0.2066,  1.6075,  ..., -0.3634, -1.0415, -0.5196]],\n\n        [[-0.7404, -0.9785,  0.6686,  ..., -0.3187,  0.7817,  0.4805],\n         [ 0.3898,  0.8919, -0.4051,  ..., -2.0851, -0.6838,  0.3581],\n         [ 1.9551, -0.9618, -2.0367,  ..., -1.0374, -2.8076,  0.8366],\n         ...,\n         [ 0.4614, -1.7450, -0.4548,  ..., -0.6019, -0.4141,  0.2776],\n         [ 0.4524, -0.4429, -0.1101,  ...,  0.1132,  1.5832,  0.0119],\n         [ 0.7627,  1.4199,  1.7429,  ...,  0.6322, -0.5401,  2.2322]]])\nenc_output: tensor([[[-0.7681,  0.5436,  0.0436,  ...,  0.3533,  0.1035,  1.4445],\n         [ 0.3921, -0.5069, -1.7588,  ...,  1.2626,  1.8381, -0.3348],\n         [ 0.9103, -0.1582,  1.9106,  ..., -0.3475, -1.4060,  1.5938],\n         ...,\n         [ 2.1004,  0.4837, -0.9899,  ..., -0.4019,  1.1672, -0.4940],\n         [-1.7220, -0.2943, -0.3310,  ...,  0.1691,  1.2841,  0.3609],\n         [-1.5807,  0.3208,  0.4989,  ...,  1.1838, -0.5952,  0.1470]],\n\n        [[ 0.2372,  1.2076,  0.1935,  ..., -0.4128,  0.7220,  1.7605],\n         [ 1.0690, -0.0378,  1.0549,  ...,  0.9890, -0.6004,  0.1620],\n         [-2.1574,  0.2065,  1.6855,  ..., -0.5804, -1.7403,  0.6199],\n         ...,\n         [-0.9273,  0.6014,  1.6834,  ...,  1.0189,  0.3023, -0.1203],\n         [-0.6820, -0.4105, -3.1790,  ..., -0.0516,  1.3272, -0.8542],\n         [ 0.1490,  0.3611, -0.2116,  ..., -0.2738, -0.0633,  0.8493]]])\nsrc_mask: tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n\n\n        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\ntgt_mask: tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\nSelf Attention Output Shape: torch.Size([2, 10, 512])\nOutput After Norm1: torch.Size([2, 10, 512])\nCross Attention Output Shape: torch.Size([2, 10, 512])\nOutput After Norm2: torch.Size([2, 10, 512])\nFeed Forward Output Shape: torch.Size([2, 10, 512])\nFinal Output Shape After Norm3: torch.Size([2, 10, 512])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 🔹 Combining the Encoder and Decoder layers to create the complete Transformer network\n\n<p align=\"center\">\n  <img src=\"https://cdn.prod.website-files.com/62c4a9809a85693c49c4674f/6580badb5a5031ccde99abac_transformer-model.png\" width=\"600\"/>/>\n</p>\n\n","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n        super(Transformer, self).__init__()\n        #  Embeddings + Positional Encoding:\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model) ## Embeddings from Module\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n\n        #loop through multiple layers as it is in paper\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n        self.fc = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def generate_mask(self, src, tgt):\n        # src_mask: Hides padding tokens in the input.\n        # tgt_mask: Prevents the decoder from looking at future tokens (no-peek).\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n        seq_length = tgt.size(1)\n        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n        tgt_mask = tgt_mask & nopeak_mask\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n\n        output = self.fc(dec_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:21:23.297193Z","iopub.execute_input":"2025-04-16T12:21:23.297484Z","iopub.status.idle":"2025-04-16T12:21:23.304981Z","shell.execute_reply.started":"2025-04-16T12:21:23.297461Z","shell.execute_reply":"2025-04-16T12:21:23.304157Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# 🚀 Step 1: Create Transformer Model\nmodel = Transformer(\n    src_vocab_size=10,\n    tgt_vocab_size=10,\n    d_model=512,\n    num_heads=8,\n    num_layers=2,\n    d_ff=2048,\n    max_seq_length=10,\n    dropout=0.1\n)\n\n# 🚀 Step 2: Create Dummy Tokenized Input\nsrc = torch.tensor([[1, 2, 3, 4]])   # shape: (1, 4) = batch_size, seq_length\ntgt = torch.tensor([[1, 2, 3]])      # shape: (1, 3)\nprint(src, tgt)\n\n# 🚀 Step 3: Generate Masks\nsrc_mask, tgt_mask = model.generate_mask(src, tgt)\nprint(\"SRC Mask:\", src_mask.shape)   # (1, 1, 1, 4)\nprint(\"TGT Mask:\", tgt_mask.shape)   # (1, 1, 3, 3)\n\n# 🚀 Step 4: Embed + Positional Encoding\nsrc_emb = model.encoder_embedding(src)                     # (1, 4, 512)\nsrc_emb = model.positional_encoding(src_emb)               # (1, 4, 512)\nsrc_emb = model.dropout(src_emb)\n\ntgt_emb = model.decoder_embedding(tgt)                     # (1, 3, 512)\ntgt_emb = model.positional_encoding(tgt_emb)               # (1, 3, 512)\ntgt_emb = model.dropout(tgt_emb)\n\n# 🚀 Step 5: Encoder Pass\nenc_output = src_emb\nfor enc_layer in model.encoder_layers:\n    enc_output = enc_layer(enc_output, src_mask)\nprint(\"Encoder Output:\", enc_output.shape)  # (1, 4, 512)\n\n# 🚀 Step 6: Decoder Pass\ndec_output = tgt_emb\nfor dec_layer in model.decoder_layers:\n    dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\nprint(\"Decoder Output:\", dec_output.shape)  # (1, 3, 512)\n\n# 🚀 Step 7: Final Linear Layer\noutput = model.fc(dec_output)               # (1, 3, 10)\nprint(\"Final Output (Logits):\", output.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T12:23:20.144610Z","iopub.execute_input":"2025-04-16T12:23:20.144902Z","iopub.status.idle":"2025-04-16T12:23:20.311017Z","shell.execute_reply.started":"2025-04-16T12:23:20.144881Z","shell.execute_reply":"2025-04-16T12:23:20.310153Z"}},"outputs":[{"name":"stdout","text":"tensor([[1, 2, 3, 4]]) tensor([[1, 2, 3]])\nSRC Mask: torch.Size([1, 1, 1, 4])\nTGT Mask: torch.Size([1, 1, 3, 3])\nEncoder Output: torch.Size([1, 4, 512])\nDecoder Output: torch.Size([1, 3, 512])\nFinal Output (Logits): torch.Size([1, 3, 10])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# 🔹 Training the PyTorch Transformer Model\n\n","metadata":{}},{"cell_type":"code","source":"src_vocab_size = 5000 # The size of the source vocabulary. This represents the total number of unique tokens (words or sub-words) in the source language.\ntgt_vocab_size = 5000 # The size of the target vocabulary. This represents the total number of unique tokens (words or sub-words) in the target language.\nd_model = 512 # dimensions of embeddings vector\nnum_heads = 8 # self attention heads in multi head attention.\nnum_layers = 6 # The number of encoder and decoder blocks in transformer.\nd_ff = 2048 # Hidden Layer dimension in FNN\nmax_seq_length = 100 #The maximum length of input/output sequences. This means that the input and output sequences can have up to 100 tokens. Any sequences longer than this would be truncated.\ndropout = 0.1 # dropout rate used to prevent overfitting.\n\ntransformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n\n# Generate random sample data\nsrc_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\ntgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T10:54:30.779117Z","iopub.execute_input":"2025-04-16T10:54:30.779419Z","iopub.status.idle":"2025-04-16T10:54:31.242993Z","shell.execute_reply.started":"2025-04-16T10:54:30.779395Z","shell.execute_reply":"2025-04-16T10:54:31.242288Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# random word embeddings\nsrc_data.shape, tgt_data.shape, src_data, tgt_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T10:55:00.177960Z","iopub.execute_input":"2025-04-16T10:55:00.178240Z","iopub.status.idle":"2025-04-16T10:55:00.185269Z","shell.execute_reply.started":"2025-04-16T10:55:00.178220Z","shell.execute_reply":"2025-04-16T10:55:00.184581Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"(torch.Size([64, 100]),\n torch.Size([64, 100]),\n tensor([[3118, 1677, 3564,  ..., 1398, 1970, 1946],\n         [3280,  810, 1202,  ..., 4571, 1018, 4718],\n         [4039, 1184, 3386,  ..., 4016, 4265, 2708],\n         ...,\n         [3910, 4409, 1089,  ..., 4814, 3007,  793],\n         [2734,  111, 4036,  ..., 4992, 3873, 1282],\n         [1951, 3727, 1519,  ..., 2099, 1435,  657]]),\n tensor([[1355,  652, 1298,  ..., 2618, 1007, 3646],\n         [1696, 1638, 2502,  ..., 4756, 4015, 2533],\n         [1137,  769, 2567,  ..., 2456,  253, 3202],\n         ...,\n         [2415,  145,  423,  ..., 3233, 4082,   63],\n         [4612, 4396, 4301,  ...,  329, 3771, 1609],\n         [3613,  421, 4539,  ..., 3359, 1831, 3654]]))"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"# 🔹 Training the Model","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n\ntransformer.train()\n\nfor epoch in range(100):\n    optimizer.zero_grad()\n    output = transformer(src_data, tgt_data[:, :-1])\n    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n    loss.backward()\n    optimizer.step()\n    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}